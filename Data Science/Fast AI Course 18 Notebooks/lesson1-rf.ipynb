{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Fast.ai's Machine Learning Course - Lesson 1<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-Random-Forests\" data-toc-modified-id=\"Intro-to-Random-Forests-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro to Random Forests</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-this-course\" data-toc-modified-id=\"About-this-course-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>About this course</a></span><ul class=\"toc-item\"><li><span><a href=\"#Teaching-approach\" data-toc-modified-id=\"Teaching-approach-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Teaching approach</a></span></li><li><span><a href=\"#Your-practice\" data-toc-modified-id=\"Your-practice-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Your practice</a></span></li><li><span><a href=\"#Books\" data-toc-modified-id=\"Books-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Books</a></span></li><li><span><a href=\"#Syllabus-in-brief\" data-toc-modified-id=\"Syllabus-in-brief-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Syllabus in brief</a></span></li></ul></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Imports</a></span></li></ul></li><li><span><a href=\"#Introduction-to-Blue-Book-for-Bulldozers\" data-toc-modified-id=\"Introduction-to-Blue-Book-for-Bulldozers-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction to <em>Blue Book for Bulldozers</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#About...\" data-toc-modified-id=\"About...-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>About...</a></span><ul class=\"toc-item\"><li><span><a href=\"#...our-teaching\" data-toc-modified-id=\"...our-teaching-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>...our teaching</a></span></li><li><span><a href=\"#...our-approach-to-machine-learning\" data-toc-modified-id=\"...our-approach-to-machine-learning-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>...our approach to machine learning</a></span></li><li><span><a href=\"#...this-dataset\" data-toc-modified-id=\"...this-dataset-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>...this dataset</a></span></li><li><span><a href=\"#...Kaggle-Competitions\" data-toc-modified-id=\"...Kaggle-Competitions-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>...Kaggle Competitions</a></span></li></ul></li><li><span><a href=\"#The-data\" data-toc-modified-id=\"The-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Look-at-the-data\" data-toc-modified-id=\"Look-at-the-data-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Look at the data</a></span></li><li><span><a href=\"#Initial-processing\" data-toc-modified-id=\"Initial-processing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Initial processing</a></span></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Pre-processing</a></span></li></ul></li></ul></li><li><span><a href=\"#Random-Forests\" data-toc-modified-id=\"Random-Forests-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Random Forests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Base-model\" data-toc-modified-id=\"Base-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Base model</a></span></li><li><span><a href=\"#Speeding-things-up\" data-toc-modified-id=\"Speeding-things-up-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Speeding things up</a></span></li><li><span><a href=\"#Single-tree\" data-toc-modified-id=\"Single-tree-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Single tree</a></span></li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Bagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-bagging\" data-toc-modified-id=\"Intro-to-bagging-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Intro to bagging</a></span></li><li><span><a href=\"#Out-of-bag-(OOB)-score\" data-toc-modified-id=\"Out-of-bag-(OOB)-score-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Out-of-bag (OOB) score</a></span></li></ul></li><li><span><a href=\"#Reducing-over-fitting\" data-toc-modified-id=\"Reducing-over-fitting-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Reducing over-fitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Subsampling\" data-toc-modified-id=\"Subsampling-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Subsampling</a></span></li><li><span><a href=\"#Tree-building-parameters\" data-toc-modified-id=\"Tree-building-parameters-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Tree building parameters</a></span></li></ul></li></ul></li><li><span><a href=\"#Additional-notes\" data-toc-modified-id=\"Additional-notes-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Additional notes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: This notebook will only work with fastai-0.7.x. Do not try to run any fastai-1.x code from this path in the repository because it will load fastai-0.7.x**\n",
    "\n",
    "> This is my revised and extended version of the first notebook from the [Fast.ai ML course (2018)](http://course18.fast.ai/ml). It covers lessons 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course is being taught by Jeremy Howard, and was developed by Jeremy along with Rachel Thomas. Rachel has been dealing with a life-threatening illness so will not be teaching as originally planned this year.\n",
    "\n",
    "Jeremy has worked in a number of different areas - feel free to ask about anything that he might be able to help you with at any time, even if not directly related to the current topic:\n",
    "\n",
    "- Management consultant (McKinsey; AT Kearney)\n",
    "- Self-funded startup entrepreneur (Fastmail: first consumer synchronized email; Optimal Decisions: first optimized insurance pricing)\n",
    "- VC-funded startup entrepreneur: (Kaggle; Enlitic: first deep-learning medical company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using a *top-down* teaching method, which is different from how most math courses operate.  Typically, in a *bottom-up* approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures.  The problems with this are that students often lose motivation, don't have a sense of the \"big picture\", and don't know what they'll need.\n",
    "\n",
    "If you took the fast.ai deep learning course, that is what we used.  You can hear more about my teaching philosophy [in this blog post](http://www.fast.ai/2016/10/08/teaching-philosophy/) or [in this talk](https://vimeo.com/214233053).\n",
    "\n",
    "Harvard Professor David Perkins has a book, [Making Learning Whole](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719) in which he uses baseball as an analogy.  We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game.  Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.\n",
    "\n",
    "All that to say, don't worry if you don't understand everything at first!  You're not supposed to.  We will start using some \"black boxes\" such as random forests that haven't yet been explained in detail, and then we'll dig into the lower level details later.\n",
    "\n",
    "To start, focus on what things DO, not what they ARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People learn by:\n",
    "1. **doing** (coding and building)\n",
    "2. **explaining** what they've learned (by writing or helping others)\n",
    "\n",
    "Therefore, we suggest that you practice these skills on Kaggle by:\n",
    "1. Entering competitions (*doing*)\n",
    "2. Creating Kaggle kernels (*explaining*)\n",
    "\n",
    "It's OK if you don't get good competition ranks or any kernel votes at first - that's totally normal! Just try to keep improving every day, and you'll see the results over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better at technical writing, study the top ranked Kaggle kernels from past competitions, and read posts from well-regarded technical bloggers. Some good role models include:\n",
    "\n",
    "- [Peter Norvig](http://nbviewer.jupyter.org/url/norvig.com/ipython/ProbabilityParadox.ipynb) (more [here](http://norvig.com/ipython/))\n",
    "- [Stephen Merity](https://smerity.com/articles/2017/deepcoder_and_ai_hype.html)\n",
    "- [Julia Evans](https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture) (more [here](https://jvns.ca/blog/2014/08/12/what-happens-if-you-write-a-tcp-stack-in-python/))\n",
    "- [Julia Ferraioli](http://blog.juliaferraioli.com/2016/02/exploring-world-using-vision-twilio.html)\n",
    "- [Edwin Chen](http://blog.echen.me/2014/10/07/moving-beyond-ctr-better-recommendations-through-human-evaluation/)\n",
    "- [Slav Ivanov](https://blog.slavv.com/picking-an-optimizer-for-style-transfer-86e7b8cba84b) (fast.ai student)\n",
    "- [Brad Kenstler](https://hackernoon.com/non-artistic-style-transfer-or-how-to-draw-kanye-using-captain-picards-face-c4a50256b814) (fast.ai and USF MSAN student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more familiarity you have with numeric programming in Python, the better. If you're looking to improve in this area, we strongly suggest Wes McKinney's [Python for Data Analysis, 2nd ed](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/ref=asap_bc?ie=UTF8).\n",
    "\n",
    "For machine learning with Python, we recommend:\n",
    "\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Andreas-Mueller/dp/1449369413): From one of the scikit-learn authors, which is the main library we'll be using\n",
    "- [Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow, 2nd Edition](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939/ref=dp_ob_title_bk): New version of a very successful book. A lot of the new material however covers deep learning in Tensorflow, which isn't relevant to this course\n",
    "- [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=MBV2QMFH3EZ6B3YBY40K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabus in brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on time and class interests, we'll cover something like (not necessarily in this order):\n",
    "\n",
    "- Train vs test\n",
    "  - Effective validation set construction\n",
    "- Trees and ensembles\n",
    "  - Creating random forests\n",
    "  - Interpreting random forests\n",
    "- What is ML?  Why do we use it?\n",
    "  - What makes a good ML project?\n",
    "  - Structured vs unstructured data\n",
    "  - Examples of failures/mistakes\n",
    "- Feature engineering\n",
    "  - Domain specific - dates, URLs, text\n",
    "  - Embeddings / latent factors\n",
    "- Regularized models trained with SGD\n",
    "  - GLMs, Elasticnet, etc (NB: see what James covered)\n",
    "- Basic neural nets\n",
    "  - PyTorch\n",
    "  - Broadcasting, Matrix Multiplication\n",
    "  - Training loop, backpropagation\n",
    "- KNN\n",
    "- CV / bootstrap (Diabetes data set?)\n",
    "- Ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip:\n",
    "\n",
    "- Dimensionality reduction\n",
    "- Interactions\n",
    "- Monitoring training\n",
    "- Collaborative filtering\n",
    "- Momentum and LR annealing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "dot = Digraph(comment='tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"8pt\" height=\"8pt\"\r\n",
       " viewBox=\"0.00 0.00 8.00 8.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 4)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-4 4,-4 4,4 -4,4\"/>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x169665eb348>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *     # We are data scientists prototyping models. This is not production code!\n",
    "from fastai.structured import *  # Importing everything and having everything available is quicker and more useful\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += os.pathsep + r'C:\\Users\\Yago\\.conda\\envs\\fastai\\Library\\bin\\graphviz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/bulldozers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to *Blue Book for Bulldozers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...our teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At fast.ai we have a distinctive [teaching philosophy](http://www.fast.ai/2016/10/08/teaching-philosophy/) of [\"the whole game\"](https://www.amazon.com/Making-Learning-Whole-Principles-Transform/dp/0470633719/ref=sr_1_1?ie=UTF8&qid=1505094653).  This is different from how most traditional math & technical courses are taught, where you have to learn all the individual elements before you can combine them (Harvard professor David Perkins call this *elementitis*), but it is similar to how topics like *driving* and *baseball* are taught.  That is, you can start driving without [knowing how an internal combustion engine works](https://medium.com/towards-data-science/thoughts-after-taking-the-deeplearning-ai-courses-8568f132153), and children begin playing baseball before they learn all the formal rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...our approach to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning courses will throw at you dozens of different algorithms, with a brief technical description of the math behind them, and maybe a toy example. You're left confused by the enormous range of techniques shown and have little practical understanding of how to apply them.\n",
    "\n",
    "The good news is that modern machine learning can be distilled down to a couple of key techniques that are of very wide applicability. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods:\n",
    "\n",
    "- *Ensembles of decision trees* (i.e. Random Forests and Gradient Boosting Machines), mainly for structured data (such as you might find in a database table at most companies)\n",
    "- *Multi-layered neural networks learnt with SGD* (i.e. shallow and/or deep learning), mainly for unstructured data (such as audio, vision, and natural language)\n",
    "\n",
    "In this course we'll be doing a deep dive into random forests, and simple models learnt with SGD. You'll be learning about gradient boosting and deep learning in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the Blue Book for Bulldozers Kaggle Competition: \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n",
    "\n",
    "This is a very common type of dataset and prediciton problem, and similar to what you may see in your project or workplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...Kaggle Competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills.  There is nothing like being able to get hands-on practice and receiving real-time feedback to help you improve your skills.\n",
    "\n",
    "Kaggle provides:\n",
    "\n",
    "1. Interesting data sets\n",
    "2. Feedback on how you're doing\n",
    "3. A leader board to see what's good, what's possible, and what's state-of-art.\n",
    "4. Blog posts by winning contestants share useful tips and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https://www.kaggle.com/c/bluebook-for-bulldozers/data) page they say the following:\n",
    "\n",
    "For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n",
    "\n",
    "- **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012. You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
    "- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n",
    "\n",
    "The key fields are in train.csv are:\n",
    "\n",
    "- SalesID: the unique identifier of the sale\n",
    "- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n",
    "- saleprice: what the machine sold for at auction (only provided in train.csv) [TARGET]\n",
    "- saledate: the date of the sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*\n",
    "\n",
    "What stands out to you from the above description?  What needs to be true of our training and validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(f'{PATH}Train.csv', low_memory=False, \n",
    "                     parse_dates=['saledate'])  # use this for all columns with dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any sort of data science work, it's **important to look at your data**, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.tail().T)  # Use transposing when you have so many columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some books make EDA at this point, but for Jeremy it's not so important.\n",
    "\n",
    "It's important to note what metric is being used for a project. Generally, selecting the metric(s) is an important part of the project setup. However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) between the actual and predicted auction prices. \n",
    "\n",
    "> $RMSE=\\sqrt{{1\\over N} \\sum(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "> $RMSLE=\\sqrt{{1\\over N} \\sum(ln(y_i+1) - ln(\\hat{y_i}+1))^2}$\n",
    "\n",
    "Therefore we take the natural logarithm of the prices, so that RMSE will give us what we need (RMSLE is RMSE using the logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw['SalePrice'].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.SalePrice = np.log(df_raw.SalePrice)\n",
    "\n",
    "display_all(df_raw['SalePrice'].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is supposed to fail due to string values in the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "\n",
    "try:\n",
    "    m.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)\n",
    "except (ValueError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a mix of **continuous** and **categorical** variables. Feature engineering is needed!!!\n",
    "\n",
    "We have a column holding the sale date, and that's important for how we work with our data. Because in a while we'll have the need to divide the dataset in training and validating subsets. And we want the validation samples to be the newest ones, since the test dataset holds future samples. So we'll order the dataframe by date! this way we'll only have to split the dataset in order to divide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.sort_values(by=['saledate'], inplace=True, ascending=True)\n",
    "df_raw['saledate'].tail(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following fastai's method extracts particular date fields from a complete datetime for the purpose of constructing categoricals. You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities.\n",
    "\n",
    "Note: if you are predicting soda sales in summer you would like to know when there is a baseball game for example; that info is very important and you won't get it without using more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??add_datepart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(df_raw, 'saledate')\n",
    "df_raw['saleYear'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical variables are currently stored as strings, which is inefficient, and doesn't provide the numeric coding required for a random forest. Therefore we call `train_cats` to convert strings to pandas categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the order to use for categorical variables if we wish (though it's not necessary; RF will solve that with more splits :D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)\n",
    "df_raw.UsageBand.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, pandas will continue displaying the text categories, while treating them as numerical data internally. Optionally, we can replace the text categories with numbers, which will make this variable non-categorical, like so:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand = df_raw.UsageBand.cat.codes\n",
    "df_raw.UsageBand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still not quite done - for instance we have lots of missing values, which we can't pass directly to a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.isnull().sum().sort_index()/len(df_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's save this dataframe to a file for now, since it's already in format can we be stored and accessed efficiently. First we'll have to reset the index, since we have reordered the dataset by sale date (saving the data frame using feather format will fail if we don't do that).\n",
    "\n",
    "Let's see the effect of reseting indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.reset_index(inplace=True)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Let's save the dataframe now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_raw.to_feather('tmp/bulldozers-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can simply read the dataframe using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_feather('tmp/bulldozers-raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll replace categories with their numeric codes (pandas automatically use -1 for missing values, so we'll sum 1 to every value), handle missing numeric continuous values (replacing those for the median of the column), and split the dependent variable into a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y, nas = proc_df(df_raw, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have something we can pass to a random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10) # paralellizable!\n",
    "m.fit(df, y)\n",
    "m.score(df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). See https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, an r^2 of 0.98 - that's great, right? Well, perhaps not...\n",
    "\n",
    "Possibly **the most important idea** in machine learning is that of having separate training & validation data sets. As motivation, suppose you don't divide up your data (like we just did), but instead use all of it.  And suppose you have lots of parameters:\n",
    "\n",
    "<img src=\"images/overfitting2.png\" alt=\"\" style=\"width: 70%\"/>\n",
    "\n",
    "[When is a model underfitted?](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)\n",
    "\n",
    "The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n",
    "\n",
    "This illustrates how using all our data can lead to **overfitting**. A validation set helps diagnose this problem!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n",
    "\n",
    "# We split the original train dataset into train and validation \n",
    "n_valid = 12000  # same as Kaggle's test set size\n",
    "n_trn = len(df) - n_valid\n",
    "#raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our model again, this time with separate training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, actuals): \n",
    "    return math.sqrt(((predictions - actuals)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    # RMSE for training, RMSE for validation, r^2 (accuracy) for training, r^2 for validation\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An r^2 in the high-80's isn't bad at all (and the RMSLE puts us around rank 100 of 470 on the Kaggle leaderboard), but we can see from the validation set score that we're over-fitting badly. To understand this issue, let's simplify things down to a single small tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding things up\n",
    "Let's use less observations, so time will reduce (and accuracy too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 30.000 random observations\n",
    "df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000, na_dict=nas)\n",
    "\n",
    "# We'll use the same validation dataset as before, so we can compare results.\n",
    "# Let's discard enough observations then; this way we'll avoid overlap\n",
    "X_train, _ = split_vals(df_trn, 20000)\n",
    "y_train, _ = split_vals(y_trn, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single tree\n",
    "What if we use a single tree, limiting its depth? error will raise for sure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap=False implies we are not doing bagging yet, only \"pasting\"\n",
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can draw this model!\n",
    "#draw_tree(m.estimators_[0], df_trn, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/tree.png](images/tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each box has:\n",
    "* the condition to branch\n",
    "* **samples**: number of samples at each point\n",
    "* **value**: mean of the log of SalePrice for the box. When we have a new input sample, the tree will decide the leaf where it belongs, and the logarithm of its sale price will be the average calculated for that leaf.\n",
    "* **mse**: RMSE if we used the prior value as the predicted result for all the samples in the box.\n",
    "\n",
    "The tree is built following one principle: at each iteration, the division must the best out of all the available ones. So the chosen variable and value is the one that minimizes the RMSE of the next level of boxes (mse1 x samples1 + mse2 x samples2).\n",
    "\n",
    "What we can think is that we'd need more depth to reduce the mse in the leafs.\n",
    "\n",
    "Let's see what happens if we create a bigger tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set result looks great! (we have one leaf per observation). But the result for the validation set is worse than for our original model. This is why we needed to use *bagging* of multiple trees, in order to get more generalizable results. Let's go back into the forest..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of bagging (or *bootstrap aggregating*) is to make any algorithm more robust. What if we create different models using random subsets, not correlated at all (built with different insights/relationships in the data), and let them give predictions for the same input. The result will predict the value as the average of the different model predictions (so we have all the insights). This averaging is a technique of *ensembling*.\n",
    "\n",
    "If we build a lot of deep decision trees, each one working on random picked data from the original set, they will overfit badly (are perfect for its subset, and crappy for the rest). Each tree will have errors, but these errors are random. And what's the average of a bunch of random errors? zero! What is left is the true relationship in the data :) That's Random Forest; a way of bagging trees (predictive but, the most important, poorly correlated trees).\n",
    "\n",
    "In Scikit-Learn each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set (63,2% of the original dataset samples will be represented in the input, which will have the same size as that dataset, fulfilling the rest with replicated samples). In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.\n",
    "\n",
    "To learn about bagging in random forests, let's start with our basic model again (working with 20.000 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap parameter is by default True, so bagging is in!\n",
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab the predictions for each individual tree, accesible through `m.estimators_`, and look at one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n",
    "preds[:,0], np.mean(preds[:,0]), y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the average of the predictions is better than the majority of the 10 individual predictions. We now have the 12.000 predicted values for each of the 10 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the R^2 score for the validation dataset using an increasing number of trees, to view how this measure evolves. The parameters for the r2_score function are: the real values for y, and the mean of the predicted values for the used trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this curve suggests that adding more trees isn't going to help us much (never going to get worse though). Let's check it! Compare this to our original model on a sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first parameter we have tried to tune. The usual is to work with 10 to 40 trees (time counts!) until optimization is required. Then you can try increasing that number, or... directly use grid search for hyper parameters optimization! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag (OOB) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is our validation set worse than our training set because we're over-fitting? or because the validation set is for a different time period? or maybe a bit of both? With the existing information we've shown, we can't tell. However, random forests (bagging algorithms in general) have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!). In fact its main purpose is helping us with model evaluation when we have a very small dataset for train, and we don't want to use part of it for validation.\n",
    "\n",
    "The idea is to evaluate each tree using the samples not chosen to train it; those would work as a validation set. To calculate the error in a sample we'll average the error of that sample for each tree where it was used for validation. This allows us to see whether the model is over-fitting.\n",
    "\n",
    "Obtaining that is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our validation set time difference is making an impact, as is model over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*. Let's return to using our full dataset, so that we can demonstrate the impact of this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before. Bad news is that we lose the OoB score (because of the implementation), but it's not a big problem!\n",
    "\n",
    "NOTE: we continue using bagging for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_rf_samples` Tells sklearn to use a subset of random and different samples for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(20000) # revert with reset_rf_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10, oob_score=False)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each additional tree allows the model to see more data, this approach can make additional trees more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=False)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So subsampling is a great option to work with large datasets instead of pure bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Test: What if we don't use bagging? That's called **pasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, bootstrap=False)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree building parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_rf_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a baseline for this full set to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dectree_max_depth(tree):\n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "\n",
    "    def walk(node_id):\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            left_max = 1 + walk(children_left[node_id])\n",
    "            right_max = 1 + walk(children_right[node_id])\n",
    "            return max(left_max, right_max)\n",
    "        else: # leaf\n",
    "            return 1\n",
    "\n",
    "    root_node_id = 0\n",
    "    return walk(root_node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why OoB is higher than R^2 is simply because OoB uses random samples for validation and R^2 uses the last ones of the time series; something more difficult to predict. \n",
    "\n",
    "We can get the depth for a tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=m.estimators_[0].tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree_max_depth(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with `min_samples_leaf`) that we require some minimum number of rows in every leaf node. This has two benefits:\n",
    "\n",
    "* There are less decision rules for each leaf node; simpler models should generalize better\n",
    "* The predictions are made by averaging more rows in the leaf node, resulting in less volatility\n",
    "\n",
    "Typical values: 1, 3, 5, 10, 25, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=m.estimators_[0].tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree_max_depth(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also increase the amount of variation amongst the trees by not only using a sample of rows for each tree, but by also using a sample of *columns* for each *split* (different columns in each one). We do this by specifying `max_features`, which is the proportion of features to randomly select from at each split (by default this value is 'auto'). Remember: the less correlated trees we use, the better results we'll get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical values: None (default, todas), 1 (1), 0.5 (50%), 'log2' (log2(num_features)), 'sqrt' (sqrt(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't compare our results directly with the Kaggle competition, since it used a different validation set (and we can no longer to submit to this competition) - but we can at least see that we're getting similar results to the winners based on the dataset we have.\n",
    "\n",
    "The sklearn docs [show an example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models:\n",
    "![sklearn max_features chart](http://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png)\n",
    "\n",
    "The reason why Random Forests performs so well is that decision trees are very flexible; they can split the samples over and over to model the dataset, so it adapts to pretty everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional notes\n",
    "\n",
    "We haven't used a test dataset in this notebook. In case we have one, we should also execute the next code over that in order to be consistent with the operations made to the train dataset:\n",
    "\n",
    "```\n",
    "apply_cats(df_raw_test, df_raw)\n",
    "\n",
    "X_test, _, _ = proc_df(df_raw_test, na_dict=nas)\n",
    "\n",
    "y_pred = m.predict(X_test)\n",
    "```\n",
    "\n",
    "And we should execute the `add_datepart` function too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": false,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Fast.ai's Machine Learning Course - Lesson 1",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
