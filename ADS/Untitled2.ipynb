{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv('combined_data.csv',encoding = 'unicode_escape')\n",
    "input_df = input_df.fillna('')\n",
    "input_df = input_df.replace('?','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = input_df[['Type','Product','industry','product_info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(\n",
    " feature_df, test_size=0.1, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex\n",
    "data_columns = ['product_info']\n",
    "Y_columns = ['Type','Product','industry']\n",
    "def preprocess_dataframe(input_df,data_columns,Y_columns):\n",
    "\n",
    "\n",
    "    df = input_df.loc[:,Y_columns]\n",
    "\n",
    "    df['text'] = input_df[data_columns].apply(lambda x: ' '.join(x.map(str)), axis=1)\n",
    "    df['text'] = df['text'].apply( lambda x: BeautifulSoup(str(x),'html.parser').get_text())\n",
    "\n",
    "    pattern = regex.compile('[\\W\\d_]+', regex.UNICODE)\n",
    "    df['text'] = df['text'].apply( lambda x: pattern.sub(' ',str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess_dataframe(x_train,data_columns,Y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = preprocess_dataframe(x_test,data_columns,Y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training shape (21518, 24840) float64\n",
      "Y training shape (21518, 3) float64\n",
      "X validation shape (2391, 24840) float64\n",
      "Y validation shape (2391, 3) float64\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "language_stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2) #ngram_range=(1,2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#https://stackoverflow.com/a/55742601/4634344\n",
    "vectorizer.fit(df_train['text'].apply(lambda x: np.str_(x)))\n",
    "X_train = vectorizer.transform(df_train['text'].apply(lambda x: np.str_(x)))\n",
    "\n",
    "# we need the class labels encoded into integers for functions in the pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "Y_train = oe.fit_transform(df_train[Y_columns].values.reshape(-1, 3))\n",
    "\n",
    "X_valid = vectorizer.transform(df_valid['text'].apply(lambda x: np.str_(x)))\n",
    "Y_valid = oe.transform(df_valid[Y_columns].values.reshape(-1, 3))\n",
    "\n",
    "print('X training shape', X_train.shape, X_train.dtype)\n",
    "print('Y training shape', Y_train.shape, Y_train.dtype)\n",
    "print('X validation shape', X_valid.shape, X_valid.dtype)\n",
    "print('Y validation shape', Y_valid.shape, Y_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf=ClassifierChain(SGDClassifier(random_state=0, class_weight='balanced', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, f1_score, make_scorer\n",
    "\n",
    "def concat_categories(Y):\n",
    "  return np.apply_along_axis(lambda a: str(a[0]) + '-' + str(a[1]), 1, Y)\n",
    "\n",
    "# score for predicting category_1\n",
    "def js_0(y,y_pred, **kwargs):\n",
    "  return jaccard_score(y[:,0], y_pred[:,0], average='micro')\n",
    "# score for predicting category_2\n",
    "def js_1(y,y_pred, **kwargs):\n",
    "  return jaccard_score(y[:,1], y_pred[:,1], average='micro')\n",
    "def js_2(y,y_pred, **kwargs):\n",
    "  return jaccard_score(y[:,2], y_pred[:,2], average='micro')\n",
    "\n",
    "def f1_0(y,y_pred, **kwargs):\n",
    "  return f1_score(y[:,0], y_pred[:,0], average='micro')\n",
    "def f1_1(y,y_pred, **kwargs):\n",
    "  return f1_score(y[:,1], y_pred[:,1], average='micro')\n",
    "def f1_2(y,y_pred, **kwargs):\n",
    "  return f1_score(y[:,2], y_pred[:,2], average='micro')\n",
    "\n",
    "# score for predicting 'category_1-category_2' (concatenated strings)\n",
    "def js_01(y,y_pred, **kwargs):\n",
    "  return jaccard_score(concat_categories(y), concat_categories(y_pred), average='micro')\n",
    "def f1_01(y,y_pred, **kwargs):\n",
    "  return f1_score(concat_categories(y), concat_categories(y_pred), average='micro')\n",
    "\n",
    "js_0_scorer = make_scorer(score_func=js_0, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "js_1_scorer = make_scorer(score_func=js_1, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "js_2_scorer = make_scorer(score_func=js_2, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "#js_01_scorer = make_scorer(score_func=js_01, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "f1_0_scorer = make_scorer(score_func=f1_0, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "f1_1_scorer = make_scorer(score_func=f1_1, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "f1_2_scorer = make_scorer(score_func=f1_1, greater_is_better=True, needs_proba=False, needs_threshold=False)\n",
    "\n",
    "\n",
    "\n",
    "f1_01_scorer = make_scorer(score_func=f1_01, greater_is_better=True, needs_proba=False, needs_threshold=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metric(y,y_pred, **kwargs):\n",
    "    \n",
    "    score1 = max(0,100*f1_score(y[:,2], y_pred[:,2], average='weighted'))\n",
    "    score2 = max(0,100*f1_score(y[:,0], y_pred[:,0], average='weighted'))\n",
    "    score3 = max(0,100*f1_score(y[:,1], y_pred[:,1], average='weighted'))\n",
    "    return (0.5*score1+0.3*score2+0.3*score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=SGDClassifier(alpha=0.0001, average=False,\n",
       "                                             class_weight='balanced',\n",
       "                                             early_stopping=False, epsilon=0.1,\n",
       "                                             eta0=0.0, fit_intercept=True,\n",
       "                                             l1_ratio=0.15,\n",
       "                                             learning_rate='optimal',\n",
       "                                             loss='hinge', max_iter=1000,\n",
       "                                             n_iter_no_change=5, n_jobs=-1,\n",
       "                                             penalty='l2', power_t=0.5,\n",
       "                                             random_state=0, shuffle=True,\n",
       "                                             tol=0.001, validation_fraction=0.1,\n",
       "                                             verbose=0, warm_start=False),\n",
       "                cv=None, order=None, random_state=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.07660169040503"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metric(Y_valid,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For both Level 1 and Level 2  concatenated:\n",
      "\tF1 micro (=accuracy): 0.65\n"
     ]
    }
   ],
   "source": [
    "print('For both Level 1 and Level 2  concatenated:\\n\\tF1 micro (=accuracy): {}'.format(f1_01(Y_valid,Y_pred).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the Level 1:\n",
      "\tF1 micro (=accuracy): 0.666\n"
     ]
    }
   ],
   "source": [
    "print('Just the Level 1:\\n\\tF1 micro (=accuracy): {}'.format(f1_0(Y_valid,Y_pred).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the Level 2:\n",
      "\tF1 micro (=accuracy): 0.969\n"
     ]
    }
   ],
   "source": [
    "print('Just the Level 2:\\n\\tF1 micro (=accuracy): {}'.format(f1_1(Y_valid,Y_pred).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the Level 3:\n",
      "\tF1 micro (=accuracy): 0.846\n"
     ]
    }
   ],
   "source": [
    "print('Just the Level 3:\\n\\tF1 micro (=accuracy): {}'.format(f1_2(Y_valid,Y_pred).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
